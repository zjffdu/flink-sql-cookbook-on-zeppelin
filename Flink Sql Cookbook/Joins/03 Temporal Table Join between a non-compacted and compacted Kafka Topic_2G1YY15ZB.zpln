{
  "paragraphs": [
    {
      "text": "%md\n\n\u003cbr/\u003e\n\n\u003e :bulb: In this recipe, you will see how to correctly enrich records from one Kafka topic with the corresponding records of another Kafka topic when the order of events matters. \n\nTemporal table joins take an arbitrary table (left input/probe site) and correlate each row to the corresponding rowâ€™s relevant version in a versioned table (right input/build side). \nFlink uses the SQL syntax of `FOR SYSTEM_TIME AS OF` to perform this operation. \n\nIn this recipe, we want join each transaction (`transactions`) to its correct currency rate (`currency_rates`, a versioned table) **as of the time when the transaction happened**.\nA similar example would be to join each order with the customer details as of the time when the order happened.\nThis is exactly what an event-time temporal table join does.\nA temporal table join in Flink SQL provides correct, deterministic results in the presence of out-of-orderness and arbitrary time skew between the two tables. \n\nBoth the `transactions` and `currency_rates` tables are backed by Kafka topics, but in the case of rates this topic is compacted (i.e. only the most recent messages for a given key are kept as updated rates flow in).\nRecords in `transactions` are interpreted as inserts only, and so the table is backed by the [standard Kafka connector](https://ci.apache.org/projects/flink/flink-docs-stable/dev/table/connectors/kafka.html) (`connector` \u003d `kafka`); while the records in `currency_rates` need to be interpreted as upserts based on a primary key, which requires the [Upsert Kafka connector](https://ci.apache.org/projects/flink/flink-docs-stable/dev/table/connectors/upsert-kafka.html) (`connector` \u003d `upsert-kafka`).\n",
      "user": "anonymous",
      "dateUpdated": "2021-10-08 22:57:58.073",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "results": {},
        "enabled": true,
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cbr/\u003e\n\u003cblockquote\u003e\n\u003cp\u003eğŸ’¡ In this recipe, you will see how to correctly enrich records from one Kafka topic with the corresponding records of another Kafka topic when the order of events matters.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eTemporal table joins take an arbitrary table (left input/probe site) and correlate each row to the corresponding rowâ€™s relevant version in a versioned table (right input/build side).\u003cbr /\u003e\nFlink uses the SQL syntax of \u003ccode\u003eFOR SYSTEM_TIME AS OF\u003c/code\u003e to perform this operation.\u003c/p\u003e\n\u003cp\u003eIn this recipe, we want join each transaction (\u003ccode\u003etransactions\u003c/code\u003e) to its correct currency rate (\u003ccode\u003ecurrency_rates\u003c/code\u003e, a versioned table) \u003cstrong\u003eas of the time when the transaction happened\u003c/strong\u003e.\u003cbr /\u003e\nA similar example would be to join each order with the customer details as of the time when the order happened.\u003cbr /\u003e\nThis is exactly what an event-time temporal table join does.\u003cbr /\u003e\nA temporal table join in Flink SQL provides correct, deterministic results in the presence of out-of-orderness and arbitrary time skew between the two tables.\u003c/p\u003e\n\u003cp\u003eBoth the \u003ccode\u003etransactions\u003c/code\u003e and \u003ccode\u003ecurrency_rates\u003c/code\u003e tables are backed by Kafka topics, but in the case of rates this topic is compacted (i.e. only the most recent messages for a given key are kept as updated rates flow in).\u003cbr /\u003e\nRecords in \u003ccode\u003etransactions\u003c/code\u003e are interpreted as inserts only, and so the table is backed by the \u003ca href\u003d\"https://ci.apache.org/projects/flink/flink-docs-stable/dev/table/connectors/kafka.html\"\u003estandard Kafka connector\u003c/a\u003e (\u003ccode\u003econnector\u003c/code\u003e \u003d \u003ccode\u003ekafka\u003c/code\u003e); while the records in \u003ccode\u003ecurrency_rates\u003c/code\u003e need to be interpreted as upserts based on a primary key, which requires the \u003ca href\u003d\"https://ci.apache.org/projects/flink/flink-docs-stable/dev/table/connectors/upsert-kafka.html\"\u003eUpsert Kafka connector\u003c/a\u003e (\u003ccode\u003econnector\u003c/code\u003e \u003d \u003ccode\u003eupsert-kafka\u003c/code\u003e).\u003c/p\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1614305485358_1290675305",
      "id": "paragraph_1614305485358_1290675305",
      "dateCreated": "2021-02-26 10:11:25.358",
      "dateStarted": "2021-10-08 22:57:58.074",
      "dateFinished": "2021-10-08 22:57:58.078",
      "status": "FINISHED"
    },
    {
      "text": "%md\n\nåœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œä½ å°†çœ‹åˆ°å½“æ—¶é—´çš„é¡ºåºå¾ˆé‡è¦çš„æ—¶å¦‚ä½•æ­£ç¡®çš„ä½¿ç”¨å¦ä¸€ä¸ªkafaka topic ä¸­å¯¹åº”çš„è®°å½•æ¥ä¸°å¯Œ kafka topic ä¸­çš„è®°å½•ã€‚\n\nTemporal table joins é‡‡ç”¨ä»»æ„ä¸€å¼ è¡¨(å·¦è¾¹ è¾“å…¥/æ¢é’ˆ åœ°å€)ï¼Œå°†æ¯ä¸€è¡Œè®°å½•ä¸ç‰ˆæœ¬è¡¨ï¼ˆå³è¾¹ è¾“å…¥/æ„å»º ä¾§ï¼‰ä¸­å¯¹åº”è¡Œè®°å½•çš„ç›¸å…³ç‰ˆæœ¬å…³è”ã€‚\nFlink ä½¿ç”¨ `FOR SYSTEM_TIME AS OF` SQL è¯­æ³•æ¥æ‰§è¡Œè¿™ä¸ªæ“ä½œã€‚\n\nåœ¨æœ¬ä¾‹ä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›å°†æ¯ä¸€ç¬”äº¤æ˜“ (`transactions`) ä¸**äº¤æ˜“å‘ç”Ÿæ—¶** æ­£ç¡®çš„è´§å¸æ±‡ç‡ (`currency_rates`, a versioned table)ï¼‰ç›¸å…³è”ã€‚\nä¸€ä¸ªç®€å•çš„ä¾‹å­å¯èƒ½æ˜¯å°†æ¯ç¬”å®šäºä¸è®¢å•å‘ç”Ÿæ—¶çš„é¡¾å®¢ä¿¡æ¯å…³è”ã€‚\nè¿™å°±æ˜¯ event-time temporal table join è¦åšçš„äº‹ã€‚\nFlink SQL ä¸­çš„ temporal table join å¯ä»¥åœ¨å«æœ‰æ— åºçš„å’Œä»»æ„æ—¶é—´åç§»çš„ 2 å¼ è¡¨ä¹‹é—´æä¾›æ­£ç¡®ã€ç¡®å®šçš„ç»“æœã€‚\n\n`transactions` and `currency_rates` è¡¨éƒ½ä½¿ç”¨ Kafka topics æä¾›æ•°æ®ï¼Œ ä½†æ˜¯åœ¨rates è¡¨ä¸­ topic æ˜¯å‹ç¼©çš„(å³ åªæœ‰ç»™å®šçš„ key çš„æœ€è¿‘çš„æ¶ˆæ¯è¢«ä½œä¸ºæ›´æ–°çš„rates è¢«ä¿ç•™)ã€‚\n \n`transactions` è¡¨ä¸­çš„è®°å½•è¢«ç¿»è¯‘ä¸ºåªèƒ½æ’å…¥ï¼Œæ‰€ä»¥è¿™ä¸ªè¡¨æ˜¯ç”± [æ ‡å‡† Kafka connector](https://ci.apache.org/projects/flink/flink-docs-stable/dev/table/connectors/kafka.html) (`connector` \u003d `kafka`)æä¾›æ•°æ®;è€Œ `currency_rates` è¡¨ä¸­çš„è®°å½•éœ€è¦è¢«ç¿»è¯‘åŸºäºä¸»é”®çš„upsertsï¼Œè¿™éœ€è¦ä½¿ç”¨ [Upsert Kafka connector](https://ci.apache.org/projects/flink/flink-docs-stable/dev/table/connectors/upsert-kafka.html) (`connector` \u003d `upsert-kafka`)ã€‚\n ",
      "user": "anonymous",
      "dateUpdated": "2021-03-18 15:56:27.877",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "results": {},
        "enabled": true,
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eåœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œä½ å°†çœ‹åˆ°å½“æ—¶é—´çš„é¡ºåºå¾ˆé‡è¦çš„æ—¶å¦‚ä½•æ­£ç¡®çš„ä½¿ç”¨å¦ä¸€ä¸ªkafaka topic ä¸­å¯¹åº”çš„è®°å½•æ¥ä¸°å¯Œ kafka topic ä¸­çš„è®°å½•ã€‚\u003c/p\u003e\n\u003cp\u003eTemporal table joins é‡‡ç”¨ä»»æ„ä¸€å¼ è¡¨(å·¦è¾¹ è¾“å…¥/æ¢é’ˆ åœ°å€)ï¼Œå°†æ¯ä¸€è¡Œè®°å½•ä¸ç‰ˆæœ¬è¡¨ï¼ˆå³è¾¹ è¾“å…¥/æ„å»º ä¾§ï¼‰ä¸­å¯¹åº”è¡Œè®°å½•çš„ç›¸å…³ç‰ˆæœ¬å…³è”ã€‚\u003cbr /\u003e\nFlink ä½¿ç”¨ \u003ccode\u003eFOR SYSTEM_TIME AS OF\u003c/code\u003e SQL è¯­æ³•æ¥æ‰§è¡Œè¿™ä¸ªæ“ä½œã€‚\u003c/p\u003e\n\u003cp\u003eåœ¨æœ¬ä¾‹ä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›å°†æ¯ä¸€ç¬”äº¤æ˜“ (\u003ccode\u003etransactions\u003c/code\u003e) ä¸\u003cstrong\u003eäº¤æ˜“å‘ç”Ÿæ—¶\u003c/strong\u003e æ­£ç¡®çš„è´§å¸æ±‡ç‡ (\u003ccode\u003ecurrency_rates\u003c/code\u003e, a versioned table)ï¼‰ç›¸å…³è”ã€‚\u003cbr /\u003e\nä¸€ä¸ªç®€å•çš„ä¾‹å­å¯èƒ½æ˜¯å°†æ¯ç¬”å®šäºä¸è®¢å•å‘ç”Ÿæ—¶çš„é¡¾å®¢ä¿¡æ¯å…³è”ã€‚\u003cbr /\u003e\nè¿™å°±æ˜¯ event-time temporal table join è¦åšçš„äº‹ã€‚\u003cbr /\u003e\nFlink SQL ä¸­çš„ temporal table join å¯ä»¥åœ¨å«æœ‰æ— åºçš„å’Œä»»æ„æ—¶é—´åç§»çš„ 2 å¼ è¡¨ä¹‹é—´æä¾›æ­£ç¡®ã€ç¡®å®šçš„ç»“æœã€‚\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003etransactions\u003c/code\u003e and \u003ccode\u003ecurrency_rates\u003c/code\u003e è¡¨éƒ½ä½¿ç”¨ Kafka topics æä¾›æ•°æ®ï¼Œ ä½†æ˜¯åœ¨rates è¡¨ä¸­ topic æ˜¯å‹ç¼©çš„(å³ åªæœ‰ç»™å®šçš„ key çš„æœ€è¿‘çš„æ¶ˆæ¯è¢«ä½œä¸ºæ›´æ–°çš„rates è¢«ä¿ç•™)ã€‚\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003etransactions\u003c/code\u003e è¡¨ä¸­çš„è®°å½•è¢«ç¿»è¯‘ä¸ºåªèƒ½æ’å…¥ï¼Œæ‰€ä»¥è¿™ä¸ªè¡¨æ˜¯ç”± \u003ca href\u003d\"https://ci.apache.org/projects/flink/flink-docs-stable/dev/table/connectors/kafka.html\"\u003eæ ‡å‡† Kafka connector\u003c/a\u003e (\u003ccode\u003econnector\u003c/code\u003e \u003d \u003ccode\u003ekafka\u003c/code\u003e)æä¾›æ•°æ®;è€Œ \u003ccode\u003ecurrency_rates\u003c/code\u003e è¡¨ä¸­çš„è®°å½•éœ€è¦è¢«ç¿»è¯‘åŸºäºä¸»é”®çš„upsertsï¼Œè¿™éœ€è¦ä½¿ç”¨ \u003ca href\u003d\"https://ci.apache.org/projects/flink/flink-docs-stable/dev/table/connectors/upsert-kafka.html\"\u003eUpsert Kafka connector\u003c/a\u003e (\u003ccode\u003econnector\u003c/code\u003e \u003d \u003ccode\u003eupsert-kafka\u003c/code\u003e)ã€‚\u003c/p\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1615193779157_760701183",
      "id": "paragraph_1615193779157_760701183",
      "dateCreated": "2021-03-08 08:56:19.157",
      "dateStarted": "2021-03-18 15:56:27.878",
      "dateFinished": "2021-03-18 15:56:27.894",
      "status": "FINISHED"
    },
    {
      "text": "%flink.ssql\n\nDROP TABLE IF EXISTS currency_rates;\n\nCREATE TABLE currency_rates (\n  `currency_code` STRING,\n  `eur_rate` DECIMAL(6,4),\n  `rate_time` TIMESTAMP(3),\n  WATERMARK FOR `rate_time` AS rate_time - INTERVAL \u002715\u0027 SECONDS,\n  PRIMARY KEY (currency_code) NOT ENFORCED\n) WITH (\n  \u0027connector\u0027 \u003d \u0027upsert-kafka\u0027,\n  \u0027topic\u0027 \u003d \u0027currency_rates\u0027,\n  \u0027properties.bootstrap.servers\u0027 \u003d \u0027localhost:9092\u0027,\n  \u0027key.format\u0027 \u003d \u0027raw\u0027,\n  \u0027value.format\u0027 \u003d \u0027json\u0027\n);\n\nDROP TABLE IF EXISTS transactions;\n\nCREATE TABLE transactions (\n  `id` STRING,\n  `currency_code` STRING,\n  `total` DECIMAL(10,2),\n  `transaction_time` TIMESTAMP(3),\n  WATERMARK FOR `transaction_time` AS transaction_time - INTERVAL \u002730\u0027 SECONDS\n) WITH (\n  \u0027connector\u0027 \u003d \u0027kafka\u0027,\n  \u0027topic\u0027 \u003d \u0027transactions\u0027,\n  \u0027properties.bootstrap.servers\u0027 \u003d \u0027localhost:9092\u0027,\n  \u0027key.format\u0027 \u003d \u0027raw\u0027,\n  \u0027key.fields\u0027 \u003d \u0027id\u0027,\n  \u0027value.format\u0027 \u003d \u0027json\u0027,\n  \u0027value.fields-include\u0027 \u003d \u0027ALL\u0027\n);\n",
      "user": "anonymous",
      "dateUpdated": "2021-03-08 09:48:31.749",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/sql",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Table has been dropped.\nTable has been created.\nTable has been dropped.\nTable has been created.\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1614410222972_538749509",
      "id": "paragraph_1614410222972_538749509",
      "dateCreated": "2021-02-27 15:17:02.972",
      "dateStarted": "2021-03-08 09:48:31.833",
      "dateFinished": "2021-03-08 09:48:34.395",
      "status": "FINISHED"
    },
    {
      "text": "%flink.ssql(type\u003dupdate)\n\nSELECT \n  t.id,\n  t.total * c.eur_rate AS total_eur,\n  t.total, \n  c.currency_code,\n  t.transaction_time\nFROM transactions t\nJOIN currency_rates FOR SYSTEM_TIME AS OF t.transaction_time AS c\nON t.currency_code \u003d c.currency_code;\nORDER BY t.transaction_time DESC\nLIMIT 10\n",
      "user": "anonymous",
      "dateUpdated": "2021-03-08 09:48:42.897",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/sql",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "Fail to run sql command: SELECT \n  t.id,\n  t.total * c.eur_rate AS total_eur,\n  t.total, \n  c.currency_code,\n  t.transaction_time\nFROM transactions t\nJOIN currency_rates FOR SYSTEM_TIME AS OF t.transaction_time AS c\nON t.currency_code \u003d c.currency_code\norg.apache.flink.table.api.ValidationException: Unable to create a source for reading table \u0027default_catalog.default_database.transactions\u0027.\n\nTable options are:\n\n\u0027connector\u0027\u003d\u0027kafka\u0027\n\u0027key.fields\u0027\u003d\u0027id\u0027\n\u0027key.format\u0027\u003d\u0027raw\u0027\n\u0027properties.bootstrap.servers\u0027\u003d\u0027localhost:9092\u0027\n\u0027topic\u0027\u003d\u0027transactions\u0027\n\u0027value.fields-include\u0027\u003d\u0027ALL\u0027\n\u0027value.format\u0027\u003d\u0027json\u0027\n\tat org.apache.flink.table.factories.FactoryUtil.createTableSource(FactoryUtil.java:122)\n\tat org.apache.flink.table.planner.plan.schema.CatalogSourceTable.createDynamicTableSource(CatalogSourceTable.java:254)\n\tat org.apache.flink.table.planner.plan.schema.CatalogSourceTable.toRel(CatalogSourceTable.java:100)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.toRel(SqlToRelConverter.java:3585)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertIdentifier(SqlToRelConverter.java:2507)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2144)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2093)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2050)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertJoin(SqlToRelConverter.java:2864)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2162)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2050)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:663)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:644)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3438)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:570)\n\tat org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:165)\n\tat org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:157)\n\tat org.apache.flink.table.planner.operations.SqlToOperationConverter.toQueryOperation(SqlToOperationConverter.java:902)\n\tat org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlQuery(SqlToOperationConverter.java:871)\n\tat org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:250)\n\tat org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:77)\n\tat org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:640)\n\tat org.apache.zeppelin.flink.sql.AbstractStreamSqlJob.run(AbstractStreamSqlJob.java:102)\n\tat org.apache.zeppelin.flink.FlinkStreamSqlInterpreter.callInnerSelect(FlinkStreamSqlInterpreter.java:89)\n\tat org.apache.zeppelin.flink.FlinkSqlInterrpeter.callSelect(FlinkSqlInterrpeter.java:494)\n\tat org.apache.zeppelin.flink.FlinkSqlInterrpeter.callCommand(FlinkSqlInterrpeter.java:257)\n\tat org.apache.zeppelin.flink.FlinkSqlInterrpeter.runSqlList(FlinkSqlInterrpeter.java:151)\n\tat org.apache.zeppelin.flink.FlinkSqlInterrpeter.internalInterpret(FlinkSqlInterrpeter.java:111)\n\tat org.apache.zeppelin.interpreter.AbstractInterpreter.interpret(AbstractInterpreter.java:47)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:110)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:852)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:744)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:172)\n\tat org.apache.zeppelin.scheduler.AbstractScheduler.runJob(AbstractScheduler.java:132)\n\tat org.apache.zeppelin.scheduler.ParallelScheduler.lambda$runJobInScheduler$0(ParallelScheduler.java:46)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.flink.table.api.ValidationException: Cannot discover a connector using option: \u0027connector\u0027\u003d\u0027kafka\u0027\n\tat org.apache.flink.table.factories.FactoryUtil.enrichNoMatchingConnectorError(FactoryUtil.java:367)\n\tat org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:354)\n\tat org.apache.flink.table.factories.FactoryUtil.createTableSource(FactoryUtil.java:118)\n\t... 37 more\nCaused by: org.apache.flink.table.api.ValidationException: Could not find any factory for identifier \u0027kafka\u0027 that implements \u0027org.apache.flink.table.factories.DynamicTableFactory\u0027 in the classpath.\n\nAvailable factory identifiers are:\n\nblackhole\ndatagen\nfaker\nfilesystem\nprint\n\tat org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:235)\n\tat org.apache.flink.table.factories.FactoryUtil.enrichNoMatchingConnectorError(FactoryUtil.java:363)\n\t... 39 more\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1614410262138_1486992945",
      "id": "paragraph_1614410262138_1486992945",
      "dateCreated": "2021-02-27 15:17:42.139",
      "dateStarted": "2021-03-08 09:48:42.970",
      "dateFinished": "2021-03-08 09:48:44.074",
      "status": "ERROR"
    },
    {
      "text": "%flink.ssql\n",
      "user": "anonymous",
      "dateUpdated": "2021-02-27 15:18:15.629",
      "progress": 0,
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1614410295629_1078002664",
      "id": "paragraph_1614410295629_1078002664",
      "dateCreated": "2021-02-27 15:18:15.629",
      "status": "READY"
    }
  ],
  "name": "03 Temporal Table Join between a non-compacted and compacted Kafka Topic",
  "id": "2G1YY15ZB",
  "defaultInterpreterGroup": "flink",
  "version": "0.10.0-SNAPSHOT",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}